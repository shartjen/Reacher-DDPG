{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Single-Agent-Unity-Environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "RunMultiAgent = False\n",
    "if RunMultiAgent:\n",
    "    print('Starting Multi-Agent-Unity-Environment')\n",
    "    env_file_name = \"Reacher_Windows_x86_64_Multi/Reacher.exe\"\n",
    "    env = UnityEnvironment(file_name=env_file_name,no_graphics=True)\n",
    "else:\n",
    "    print('Starting Single-Agent-Unity-Environment')\n",
    "    env_file_name = \"Reacher_Windows_x86_64_Single/Reacher.exe\"\n",
    "    env = UnityEnvironment(file_name=env_file_name,no_graphics=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brainstorming :  ReacherBrain\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "print('Brainstorming : ',brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runrandom = False\n",
    "if runrandom:\n",
    "    for i_episode in range(20):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        i = 0\n",
    "        while True and i<1000:\n",
    "            i += 1\n",
    "            if i % 200 == 0:\n",
    "                print(i)\n",
    "            # ToDo: randn draws from standard normal distribution, do I want this or better uniform scaled to -1,1\n",
    "            actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            # print(actions)\n",
    "            # print(type(actions))\n",
    "            # print(actions.shape)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Parameters    : \n",
      "gamma                : 0.99\n",
      "tau                  : 0.001\n",
      "action_size          : 4\n",
      "state_size           : 33\n",
      "hidden_size          : 128\n",
      "buffer_size          : 100000\n",
      "batch_size           : 128\n",
      "dropout              : 0.2\n",
      "seed                 : 1\n",
      "max_episodes         : 150\n",
      "critic_learning_rate : 0.0001\n",
      "actor_learning_rate  : 0.0001\n",
      "num_agents           : 1\n",
      "env_file_name        : Reacher_Windows_x86_64_Single/Reacher.exe\n",
      "brain_name           : ReacherBrain\n",
      "Running on device :  cpu\n",
      "Episode 0 || Total Reward :  0.000 || average reward :  0.000 || Used 20.506 seconds\n",
      "Episode 1 || Total Reward :  0.430 || average reward :  0.215 || Used 24.724 seconds\n",
      "Episode 2 || Total Reward :  0.240 || average reward :  0.223 || Used 26.733 seconds\n",
      "Episode 3 || Total Reward :  0.920 || average reward :  0.397 || Used 27.077 seconds\n",
      "Episode 4 || Total Reward :  1.020 || average reward :  0.522 || Used 25.328 seconds\n",
      "Episode 5 || Total Reward :  0.000 || average reward :  0.435 || Used 20.323 seconds\n",
      "Episode 6 || Total Reward :  0.420 || average reward :  0.433 || Used 20.437 seconds\n",
      "Episode 7 || Total Reward :  1.550 || average reward :  0.572 || Used 20.537 seconds\n",
      "Episode 8 || Total Reward :  0.400 || average reward :  0.553 || Used 20.971 seconds\n",
      "Episode 9 || Total Reward :  0.210 || average reward :  0.519 || Used 20.483 seconds\n",
      "Episode 10 || Total Reward :  0.220 || average reward :  0.492 || Used 20.483 seconds\n",
      "Episode 11 || Total Reward :  0.000 || average reward :  0.451 || Used 22.951 seconds\n",
      "Episode 12 || Total Reward :  0.780 || average reward :  0.476 || Used 22.135 seconds\n",
      "Episode 13 || Total Reward :  0.240 || average reward :  0.459 || Used 20.708 seconds\n",
      "Episode 14 || Total Reward :  0.070 || average reward :  0.433 || Used 20.534 seconds\n",
      "Episode 15 || Total Reward :  1.660 || average reward :  0.510 || Used 20.532 seconds\n",
      "Episode 16 || Total Reward :  0.680 || average reward :  0.520 || Used 20.425 seconds\n",
      "Episode 17 || Total Reward :  0.730 || average reward :  0.532 || Used 21.437 seconds\n",
      "Episode 18 || Total Reward :  0.390 || average reward :  0.524 || Used 27.777 seconds\n",
      "Episode 19 || Total Reward :  0.900 || average reward :  0.543 || Used 25.789 seconds\n",
      "Episode 20 || Total Reward :  0.940 || average reward :  0.562 || Used 29.847 seconds\n",
      "Episode 21 || Total Reward :  0.860 || average reward :  0.575 || Used 27.026 seconds\n",
      "Episode 22 || Total Reward :  1.510 || average reward :  0.616 || Used 30.841 seconds\n",
      "Episode 23 || Total Reward :  1.720 || average reward :  0.662 || Used 31.453 seconds\n",
      "Episode 24 || Total Reward :  0.800 || average reward :  0.668 || Used 29.254 seconds\n",
      "Episode 25 || Total Reward :  3.130 || average reward :  0.762 || Used 29.417 seconds\n",
      "Episode 26 || Total Reward :  0.850 || average reward :  0.766 || Used 28.050 seconds\n",
      "Episode 27 || Total Reward :  2.250 || average reward :  0.819 || Used 23.695 seconds\n",
      "Episode 28 || Total Reward :  1.260 || average reward :  0.834 || Used 27.446 seconds\n",
      "Episode 29 || Total Reward :  1.150 || average reward :  0.844 || Used 26.854 seconds\n",
      "Episode 30 || Total Reward :  0.980 || average reward :  0.849 || Used 28.897 seconds\n",
      "Episode 31 || Total Reward :  1.100 || average reward :  0.857 || Used 28.432 seconds\n",
      "Episode 32 || Total Reward :  1.770 || average reward :  0.884 || Used 28.240 seconds\n",
      "Episode 33 || Total Reward :  3.420 || average reward :  0.959 || Used 26.294 seconds\n",
      "Episode 34 || Total Reward :  1.870 || average reward :  0.985 || Used 28.621 seconds\n",
      "Episode 35 || Total Reward :  2.010 || average reward :  1.013 || Used 26.268 seconds\n",
      "Episode 36 || Total Reward :  1.020 || average reward :  1.014 || Used 24.939 seconds\n",
      "Episode 37 || Total Reward :  0.450 || average reward :  0.999 || Used 24.050 seconds\n",
      "Episode 38 || Total Reward :  1.180 || average reward :  1.003 || Used 23.666 seconds\n",
      "Episode 39 || Total Reward :  1.980 || average reward :  1.028 || Used 23.639 seconds\n",
      "Episode 40 || Total Reward :  2.820 || average reward :  1.071 || Used 24.054 seconds\n",
      "Episode 41 || Total Reward :  0.410 || average reward :  1.056 || Used 24.629 seconds\n",
      "Episode 42 || Total Reward :  1.990 || average reward :  1.077 || Used 26.252 seconds\n",
      "Episode 43 || Total Reward :  0.800 || average reward :  1.071 || Used 23.881 seconds\n",
      "Episode 44 || Total Reward :  2.120 || average reward :  1.094 || Used 25.032 seconds\n",
      "Episode 45 || Total Reward :  0.910 || average reward :  1.090 || Used 23.450 seconds\n",
      "Episode 46 || Total Reward :  2.110 || average reward :  1.112 || Used 24.474 seconds\n",
      "Episode 47 || Total Reward :  1.300 || average reward :  1.116 || Used 24.487 seconds\n",
      "Episode 48 || Total Reward :  2.840 || average reward :  1.151 || Used 24.647 seconds\n",
      "Episode 49 || Total Reward :  1.850 || average reward :  1.165 || Used 24.671 seconds\n",
      "Episode 50 || Total Reward :  1.800 || average reward :  1.178 || Used 23.713 seconds\n",
      "Episode 51 || Total Reward :  3.350 || average reward :  1.219 || Used 26.398 seconds\n",
      "Episode 52 || Total Reward :  1.100 || average reward :  1.217 || Used 27.144 seconds\n",
      "Episode 53 || Total Reward :  2.670 || average reward :  1.244 || Used 26.350 seconds\n",
      "Episode 54 || Total Reward :  4.950 || average reward :  1.311 || Used 23.435 seconds\n",
      "Episode 55 || Total Reward :  1.890 || average reward :  1.322 || Used 23.621 seconds\n",
      "Episode 56 || Total Reward :  1.340 || average reward :  1.322 || Used 24.777 seconds\n",
      "Episode 57 || Total Reward :  0.860 || average reward :  1.314 || Used 25.101 seconds\n",
      "Episode 58 || Total Reward :  1.360 || average reward :  1.315 || Used 25.456 seconds\n",
      "Episode 59 || Total Reward :  1.540 || average reward :  1.319 || Used 23.027 seconds\n",
      "Episode 60 || Total Reward :  2.390 || average reward :  1.336 || Used 22.889 seconds\n",
      "Episode 61 || Total Reward :  1.510 || average reward :  1.339 || Used 22.625 seconds\n",
      "Episode 62 || Total Reward :  0.980 || average reward :  1.333 || Used 22.633 seconds\n",
      "Episode 63 || Total Reward :  2.090 || average reward :  1.345 || Used 24.442 seconds\n",
      "Episode 64 || Total Reward :  1.750 || average reward :  1.351 || Used 24.496 seconds\n",
      "Episode 65 || Total Reward :  1.260 || average reward :  1.350 || Used 29.653 seconds\n",
      "Episode 66 || Total Reward :  2.140 || average reward :  1.362 || Used 25.917 seconds\n",
      "Episode 67 || Total Reward :  4.260 || average reward :  1.404 || Used 22.792 seconds\n",
      "Episode 68 || Total Reward :  2.730 || average reward :  1.424 || Used 24.221 seconds\n",
      "Episode 69 || Total Reward :  4.930 || average reward :  1.474 || Used 22.802 seconds\n",
      "Episode 70 || Total Reward :  3.110 || average reward :  1.497 || Used 22.899 seconds\n",
      "Episode 71 || Total Reward :  4.290 || average reward :  1.536 || Used 25.521 seconds\n",
      "Episode 72 || Total Reward :  3.450 || average reward :  1.562 || Used 30.105 seconds\n",
      "Episode 73 || Total Reward :  3.520 || average reward :  1.588 || Used 31.216 seconds\n",
      "Episode 74 || Total Reward :  1.680 || average reward :  1.589 || Used 28.795 seconds\n",
      "Episode 75 || Total Reward :  1.600 || average reward :  1.590 || Used 29.263 seconds\n",
      "Episode 76 || Total Reward :  1.770 || average reward :  1.592 || Used 29.716 seconds\n",
      "Episode 77 || Total Reward :  2.300 || average reward :  1.601 || Used 27.190 seconds\n",
      "Episode 78 || Total Reward :  2.720 || average reward :  1.615 || Used 23.062 seconds\n",
      "Episode 79 || Total Reward :  2.660 || average reward :  1.628 || Used 24.707 seconds\n",
      "Episode 80 || Total Reward :  2.470 || average reward :  1.639 || Used 27.729 seconds\n",
      "Episode 81 || Total Reward :  2.660 || average reward :  1.651 || Used 23.448 seconds\n",
      "Episode 82 || Total Reward :  5.690 || average reward :  1.700 || Used 23.805 seconds\n",
      "Episode 83 || Total Reward :  1.250 || average reward :  1.694 || Used 22.491 seconds\n",
      "Episode 84 || Total Reward :  2.430 || average reward :  1.703 || Used 22.895 seconds\n",
      "Episode 85 || Total Reward :  1.630 || average reward :  1.702 || Used 23.058 seconds\n",
      "Episode 86 || Total Reward :  1.570 || average reward :  1.701 || Used 24.858 seconds\n",
      "Episode 87 || Total Reward :  1.630 || average reward :  1.700 || Used 25.725 seconds\n",
      "Episode 88 || Total Reward :  0.980 || average reward :  1.692 || Used 25.839 seconds\n",
      "Episode 89 || Total Reward :  2.400 || average reward :  1.700 || Used 24.362 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90 || Total Reward :  3.080 || average reward :  1.715 || Used 23.474 seconds\n",
      "Episode 91 || Total Reward :  2.950 || average reward :  1.728 || Used 24.203 seconds\n"
     ]
    }
   ],
   "source": [
    "from ddpg_agent import ddpg_Agent\n",
    "import cProfile\n",
    "\n",
    "config = {\n",
    "    'gamma'               : 0.99,\n",
    "    'tau'                 : 1e-3,\n",
    "    'action_size'         : action_size,\n",
    "    'state_size'          : state_size,\n",
    "    'hidden_size'         : 128,\n",
    "    'buffer_size'         : 100000,\n",
    "    'batch_size'          : 128,\n",
    "    'dropout'             : 0.2,\n",
    "    'seed'                : 1,\n",
    "    'max_episodes'        : 150,\n",
    "    'critic_learning_rate': 1e-4,\n",
    "    'actor_learning_rate' : 1e-4,\n",
    "    'num_agents'          : num_agents,\n",
    "    'env_file_name'       : env_file_name,\n",
    "    'brain_name'          : brain_name}\n",
    "\n",
    "def print_config(config):\n",
    "    print('Config Parameters    : ')\n",
    "    for c,k in config.items():\n",
    "        print('{:20s} : {}'.format(c,k))\n",
    "\n",
    "# for j in range(20):\n",
    "print_config(config)\n",
    "agent = ddpg_Agent(env, config)\n",
    "# cProfile.run(\"all_rewards = agent.train()\",'PerfStats')\n",
    "all_rewards = agent.train()\n",
    "\n",
    "if False:\n",
    "    config_alt = dict(config)\n",
    "    config_alt['dropout'] = 0.3\n",
    "    print_config(config_alt)\n",
    "    agent = ddpg_Agent(env, config_alt)\n",
    "    all_rewards = agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "plt.ion()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(all_rewards)), all_rewards)\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
