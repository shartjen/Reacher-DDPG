{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multi-Agent-Unity-Environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "RunMultiAgent = True\n",
    "if RunMultiAgent:\n",
    "    print('Starting Multi-Agent-Unity-Environment')\n",
    "    env_file_name = \"Reacher_Windows_x86_64_Multi/Reacher.exe\"\n",
    "    env = UnityEnvironment(file_name=env_file_name,no_graphics=True)\n",
    "else:\n",
    "    print('Starting Single-Agent-Unity-Environment')\n",
    "    env_file_name = \"Reacher_Windows_x86_64_Single/Reacher.exe\"\n",
    "    env = UnityEnvironment(file_name=env_file_name,no_graphics=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brainstorming :  ReacherBrain\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "print('Brainstorming : ',brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runrandom = False\n",
    "if runrandom:\n",
    "    for i_episode in range(20):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        i = 0\n",
    "        while True and i<1000:\n",
    "            i += 1\n",
    "            if i % 200 == 0:\n",
    "                print(i)\n",
    "            # ToDo: randn draws from standard normal distribution, do I want this or better uniform scaled to -1,1\n",
    "            actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            # print(actions)\n",
    "            # print(type(actions))\n",
    "            # print(actions.shape)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Parameters    : \n",
      "gamma                : 0.99\n",
      "tau                  : 0.001\n",
      "action_size          : 4\n",
      "state_size           : 33\n",
      "hidden_size          : 256\n",
      "buffer_size          : 200000\n",
      "batch_size           : 256\n",
      "dropout              : 0.2\n",
      "seed                 : 1\n",
      "max_episodes         : 256\n",
      "critic_learning_rate : 0.0001\n",
      "actor_learning_rate  : 0.0001\n",
      "num_agents           : 20\n",
      "env_file_name        : Reacher_Windows_x86_64_Multi/Reacher.exe\n",
      "brain_name           : ReacherBrain\n",
      "Running on device :  cpu\n",
      "\u001b[41mEpisode 0 || Total Reward :  0.608 || average reward :  0.608\n",
      "\u001b[0m\n",
      "Saved Networks in  trained_reacher_e0.pth\n",
      "Episode 1 || Total Reward :  0.607 || average reward :  0.608 || Used 45.870 seconds, mem : 40040\n",
      "Episode 2 || Total Reward :  0.921 || average reward :  0.712 || Used 44.511 seconds, mem : 60060\n",
      "Episode 3 || Total Reward :  0.762 || average reward :  0.725 || Used 43.665 seconds, mem : 80080\n",
      "Episode 4 || Total Reward :  0.968 || average reward :  0.773 || Used 43.457 seconds, mem : 100100\n",
      "Episode 5 || Total Reward :  0.811 || average reward :  0.780 || Used 43.521 seconds, mem : 120120\n",
      "Episode 6 || Total Reward :  0.734 || average reward :  0.773 || Used 44.023 seconds, mem : 140140\n",
      "Episode 7 || Total Reward :  0.828 || average reward :  0.780 || Used 44.518 seconds, mem : 160160\n",
      "Episode 8 || Total Reward :  0.890 || average reward :  0.792 || Used 44.762 seconds, mem : 180180\n",
      "Episode 9 || Total Reward :  0.959 || average reward :  0.809 || Used 45.458 seconds, mem : 200000\n",
      "Episode 10 || Total Reward :  1.485 || average reward :  0.870 || Used 45.736 seconds, mem : 200000\n",
      "Episode 11 || Total Reward :  1.213 || average reward :  0.899 || Used 45.899 seconds, mem : 200000\n",
      "Episode 12 || Total Reward :  1.611 || average reward :  0.954 || Used 46.792 seconds, mem : 200000\n",
      "Episode 13 || Total Reward :  1.604 || average reward :  1.000 || Used 45.085 seconds, mem : 200000\n",
      "Episode 14 || Total Reward :  1.742 || average reward :  1.050 || Used 45.477 seconds, mem : 200000\n",
      "Episode 15 || Total Reward :  1.654 || average reward :  1.087 || Used 45.895 seconds, mem : 200000\n",
      "Episode 16 || Total Reward :  2.157 || average reward :  1.150 || Used 46.078 seconds, mem : 200000\n",
      "Episode 17 || Total Reward :  2.293 || average reward :  1.214 || Used 46.012 seconds, mem : 200000\n",
      "Episode 18 || Total Reward :  2.329 || average reward :  1.273 || Used 45.799 seconds, mem : 200000\n",
      "Episode 19 || Total Reward :  2.737 || average reward :  1.346 || Used 45.625 seconds, mem : 200000\n",
      "Episode 20 || Total Reward :  5.148 || average reward :  1.527 || Used 45.746 seconds, mem : 200000\n",
      "Episode 21 || Total Reward :  4.128 || average reward :  1.645 || Used 45.493 seconds, mem : 200000\n",
      "Episode 22 || Total Reward :  4.948 || average reward :  1.789 || Used 46.520 seconds, mem : 200000\n",
      "Episode 23 || Total Reward :  5.325 || average reward :  1.936 || Used 45.499 seconds, mem : 200000\n",
      "Episode 24 || Total Reward :  5.263 || average reward :  2.069 || Used 45.561 seconds, mem : 200000\n",
      "\u001b[41mEpisode 25 || Total Reward :  5.157 || average reward :  2.188\n",
      "\u001b[0m\n",
      "Episode 26 || Total Reward :  6.320 || average reward :  2.341 || Used 45.573 seconds, mem : 200000\n",
      "Episode 27 || Total Reward :  6.566 || average reward :  2.492 || Used 45.543 seconds, mem : 200000\n",
      "Episode 28 || Total Reward :  6.491 || average reward :  2.630 || Used 45.830 seconds, mem : 200000\n",
      "Episode 29 || Total Reward :  5.972 || average reward :  2.741 || Used 44.955 seconds, mem : 200000\n",
      "Episode 30 || Total Reward :  6.085 || average reward :  2.849 || Used 44.304 seconds, mem : 200000\n",
      "Episode 31 || Total Reward :  5.613 || average reward :  2.935 || Used 44.100 seconds, mem : 200000\n",
      "Episode 32 || Total Reward :  5.737 || average reward :  3.020 || Used 45.465 seconds, mem : 200000\n",
      "Episode 33 || Total Reward :  5.878 || average reward :  3.104 || Used 46.043 seconds, mem : 200000\n",
      "Episode 34 || Total Reward :  6.813 || average reward :  3.210 || Used 74.988 seconds, mem : 200000\n",
      "Episode 35 || Total Reward :  7.438 || average reward :  3.328 || Used 63.099 seconds, mem : 200000\n",
      "Episode 36 || Total Reward :  7.697 || average reward :  3.446 || Used 52.106 seconds, mem : 200000\n",
      "Episode 37 || Total Reward :  7.729 || average reward :  3.559 || Used 52.323 seconds, mem : 200000\n",
      "Episode 38 || Total Reward :  7.030 || average reward :  3.648 || Used 52.519 seconds, mem : 200000\n",
      "Episode 39 || Total Reward :  6.262 || average reward :  3.713 || Used 52.424 seconds, mem : 200000\n",
      "Episode 40 || Total Reward :  7.129 || average reward :  3.796 || Used 52.129 seconds, mem : 200000\n",
      "Episode 41 || Total Reward :  7.034 || average reward :  3.873 || Used 52.166 seconds, mem : 200000\n",
      "Episode 42 || Total Reward :  9.135 || average reward :  3.996 || Used 52.620 seconds, mem : 200000\n",
      "Episode 43 || Total Reward :  8.024 || average reward :  4.087 || Used 52.396 seconds, mem : 200000\n",
      "Episode 44 || Total Reward :  7.560 || average reward :  4.165 || Used 52.465 seconds, mem : 200000\n",
      "Episode 45 || Total Reward :  7.025 || average reward :  4.227 || Used 51.853 seconds, mem : 200000\n",
      "Episode 46 || Total Reward :  6.783 || average reward :  4.281 || Used 52.055 seconds, mem : 200000\n",
      "Episode 47 || Total Reward :  7.592 || average reward :  4.350 || Used 49.448 seconds, mem : 200000\n",
      "Episode 48 || Total Reward :  7.326 || average reward :  4.411 || Used 49.543 seconds, mem : 200000\n",
      "Episode 49 || Total Reward :  7.401 || average reward :  4.471 || Used 49.962 seconds, mem : 200000\n",
      "\u001b[41mEpisode 50 || Total Reward :  8.247 || average reward :  4.545\n",
      "\u001b[0m\n",
      "Saved Networks in  trained_reacher_e50.pth\n",
      "Episode 51 || Total Reward :  9.871 || average reward :  4.647 || Used 49.973 seconds, mem : 200000\n",
      "Episode 52 || Total Reward :  9.613 || average reward :  4.741 || Used 49.694 seconds, mem : 200000\n",
      "Episode 53 || Total Reward : 10.877 || average reward :  4.854 || Used 49.862 seconds, mem : 200000\n",
      "Episode 54 || Total Reward : 11.195 || average reward :  4.970 || Used 49.548 seconds, mem : 200000\n",
      "Episode 55 || Total Reward : 10.922 || average reward :  5.076 || Used 49.642 seconds, mem : 200000\n",
      "Episode 56 || Total Reward : 12.184 || average reward :  5.201 || Used 49.889 seconds, mem : 200000\n",
      "Episode 57 || Total Reward : 13.846 || average reward :  5.350 || Used 49.774 seconds, mem : 200000\n",
      "Episode 58 || Total Reward : 13.894 || average reward :  5.495 || Used 49.937 seconds, mem : 200000\n",
      "Episode 59 || Total Reward : 14.788 || average reward :  5.650 || Used 49.614 seconds, mem : 200000\n",
      "Episode 60 || Total Reward : 16.007 || average reward :  5.819 || Used 49.569 seconds, mem : 200000\n",
      "Episode 61 || Total Reward : 16.029 || average reward :  5.984 || Used 49.730 seconds, mem : 200000\n",
      "Episode 62 || Total Reward : 16.148 || average reward :  6.145 || Used 49.644 seconds, mem : 200000\n",
      "Episode 63 || Total Reward : 14.186 || average reward :  6.271 || Used 49.940 seconds, mem : 200000\n",
      "Episode 64 || Total Reward : 16.270 || average reward :  6.425 || Used 49.967 seconds, mem : 200000\n",
      "Episode 65 || Total Reward : 16.301 || average reward :  6.574 || Used 49.872 seconds, mem : 200000\n",
      "Episode 66 || Total Reward : 17.914 || average reward :  6.744 || Used 49.648 seconds, mem : 200000\n",
      "Episode 67 || Total Reward : 18.573 || average reward :  6.918 || Used 49.668 seconds, mem : 200000\n",
      "Episode 68 || Total Reward : 21.152 || average reward :  7.124 || Used 50.039 seconds, mem : 200000\n",
      "Episode 69 || Total Reward : 19.105 || average reward :  7.295 || Used 50.104 seconds, mem : 200000\n",
      "Episode 70 || Total Reward : 20.702 || average reward :  7.484 || Used 50.050 seconds, mem : 200000\n",
      "Episode 71 || Total Reward : 21.472 || average reward :  7.678 || Used 49.823 seconds, mem : 200000\n",
      "Episode 72 || Total Reward : 23.399 || average reward :  7.894 || Used 49.679 seconds, mem : 200000\n",
      "Episode 73 || Total Reward : 20.925 || average reward :  8.070 || Used 49.883 seconds, mem : 200000\n",
      "Episode 74 || Total Reward : 20.518 || average reward :  8.236 || Used 50.001 seconds, mem : 200000\n",
      "\u001b[41mEpisode 75 || Total Reward : 21.081 || average reward :  8.405\n",
      "\u001b[0m\n",
      "Episode 76 || Total Reward : 22.529 || average reward :  8.588 || Used 49.674 seconds, mem : 200000\n",
      "Episode 77 || Total Reward : 18.572 || average reward :  8.716 || Used 49.623 seconds, mem : 200000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 78 || Total Reward : 21.340 || average reward :  8.876 || Used 49.977 seconds, mem : 200000\n",
      "Episode 79 || Total Reward : 24.120 || average reward :  9.066 || Used 50.078 seconds, mem : 200000\n",
      "Episode 80 || Total Reward : 25.861 || average reward :  9.274 || Used 49.969 seconds, mem : 200000\n",
      "Episode 81 || Total Reward : 25.987 || average reward :  9.478 || Used 49.651 seconds, mem : 200000\n",
      "Episode 82 || Total Reward : 27.467 || average reward :  9.694 || Used 49.736 seconds, mem : 200000\n",
      "Episode 83 || Total Reward : 26.747 || average reward :  9.897 || Used 49.954 seconds, mem : 200000\n",
      "Episode 84 || Total Reward : 28.757 || average reward : 10.119 || Used 50.125 seconds, mem : 200000\n",
      "Episode 85 || Total Reward : 29.917 || average reward : 10.349 || Used 49.902 seconds, mem : 200000\n",
      "Episode 86 || Total Reward : 29.441 || average reward : 10.569 || Used 49.834 seconds, mem : 200000\n",
      "Episode 87 || Total Reward : 26.954 || average reward : 10.755 || Used 50.340 seconds, mem : 200000\n",
      "Episode 88 || Total Reward : 27.476 || average reward : 10.943 || Used 50.460 seconds, mem : 200000\n",
      "Episode 89 || Total Reward : 29.660 || average reward : 11.151 || Used 49.978 seconds, mem : 200000\n",
      "Episode 90 || Total Reward : 33.066 || average reward : 11.392 || Used 55.329 seconds, mem : 200000\n",
      "Episode 91 || Total Reward : 32.050 || average reward : 11.616 || Used 50.088 seconds, mem : 200000\n",
      "Episode 92 || Total Reward : 34.099 || average reward : 11.858 || Used 50.252 seconds, mem : 200000\n",
      "Episode 93 || Total Reward : 32.725 || average reward : 12.080 || Used 49.972 seconds, mem : 200000\n",
      "Episode 94 || Total Reward : 36.013 || average reward : 12.332 || Used 49.998 seconds, mem : 200000\n",
      "Episode 95 || Total Reward : 35.377 || average reward : 12.572 || Used 49.759 seconds, mem : 200000\n",
      "Episode 96 || Total Reward : 30.220 || average reward : 12.754 || Used 49.978 seconds, mem : 200000\n",
      "Episode 97 || Total Reward : 31.560 || average reward : 12.946 || Used 50.532 seconds, mem : 200000\n",
      "Episode 98 || Total Reward : 29.943 || average reward : 13.118 || Used 49.982 seconds, mem : 200000\n",
      "Episode 99 || Total Reward : 29.308 || average reward : 13.280 || Used 49.688 seconds, mem : 200000\n",
      "New Memory length :  200000\n",
      "\u001b[41mEpisode 100 || Total Reward : 30.727 || average reward : 13.581\n",
      "\u001b[0m\n",
      "Saved Networks in  trained_reacher_e100.pth\n",
      "Episode 101 || Total Reward : 36.506 || average reward : 13.940 || Used 50.101 seconds, mem : 220020\n",
      "Episode 102 || Total Reward : 38.606 || average reward : 14.317 || Used 50.056 seconds, mem : 240040\n",
      "Episode 103 || Total Reward : 37.993 || average reward : 14.689 || Used 50.594 seconds, mem : 260060\n",
      "Episode 104 || Total Reward : 38.113 || average reward : 15.060 || Used 51.133 seconds, mem : 280080\n",
      "Episode 105 || Total Reward : 37.996 || average reward : 15.432 || Used 51.148 seconds, mem : 300100\n",
      "Episode 106 || Total Reward : 36.505 || average reward : 15.790 || Used 51.564 seconds, mem : 320120\n",
      "Episode 107 || Total Reward : 36.322 || average reward : 16.145 || Used 51.813 seconds, mem : 340140\n",
      "Episode 108 || Total Reward : 36.289 || average reward : 16.499 || Used 52.747 seconds, mem : 360160\n",
      "Episode 109 || Total Reward : 37.664 || average reward : 16.866 || Used 52.872 seconds, mem : 380180\n",
      "Episode 110 || Total Reward : 36.339 || average reward : 17.214 || Used 53.724 seconds, mem : 400000\n",
      "Episode 111 || Total Reward : 32.450 || average reward : 17.527 || Used 53.951 seconds, mem : 400000\n",
      "Episode 112 || Total Reward : 36.346 || average reward : 17.874 || Used 53.645 seconds, mem : 400000\n",
      "Episode 113 || Total Reward : 38.224 || average reward : 18.240 || Used 53.766 seconds, mem : 400000\n",
      "Episode 114 || Total Reward : 37.808 || average reward : 18.601 || Used 53.915 seconds, mem : 400000\n",
      "Episode 115 || Total Reward : 38.277 || average reward : 18.967 || Used 53.839 seconds, mem : 400000\n",
      "Episode 116 || Total Reward : 36.637 || average reward : 19.312 || Used 53.372 seconds, mem : 400000\n",
      "Episode 117 || Total Reward : 37.406 || average reward : 19.663 || Used 53.871 seconds, mem : 400000\n",
      "Episode 118 || Total Reward : 37.891 || average reward : 20.019 || Used 53.696 seconds, mem : 400000\n",
      "Episode 119 || Total Reward : 37.110 || average reward : 20.363 || Used 53.560 seconds, mem : 400000\n",
      "Episode 120 || Total Reward : 36.989 || average reward : 20.681 || Used 53.926 seconds, mem : 400000\n",
      "Episode 121 || Total Reward : 37.730 || average reward : 21.017 || Used 53.545 seconds, mem : 400000\n",
      "Episode 122 || Total Reward : 37.513 || average reward : 21.343 || Used 53.726 seconds, mem : 400000\n",
      "Episode 123 || Total Reward : 37.643 || average reward : 21.666 || Used 53.396 seconds, mem : 400000\n",
      "Episode 124 || Total Reward : 36.963 || average reward : 21.983 || Used 54.033 seconds, mem : 400000\n",
      "\u001b[41mEpisode 125 || Total Reward : 35.420 || average reward : 22.285\n",
      "\u001b[0m\n",
      "Episode 126 || Total Reward : 36.703 || average reward : 22.589 || Used 53.488 seconds, mem : 400000\n",
      "Episode 127 || Total Reward : 35.859 || average reward : 22.882 || Used 53.652 seconds, mem : 400000\n",
      "Episode 128 || Total Reward : 37.458 || average reward : 23.192 || Used 53.354 seconds, mem : 400000\n",
      "Episode 129 || Total Reward : 36.107 || average reward : 23.493 || Used 53.977 seconds, mem : 400000\n",
      "Episode 130 || Total Reward : 37.744 || average reward : 23.810 || Used 53.304 seconds, mem : 400000\n",
      "Episode 131 || Total Reward : 36.314 || average reward : 24.117 || Used 53.363 seconds, mem : 400000\n",
      "Episode 132 || Total Reward : 37.362 || average reward : 24.433 || Used 53.249 seconds, mem : 400000\n",
      "Episode 133 || Total Reward : 36.801 || average reward : 24.742 || Used 53.630 seconds, mem : 400000\n",
      "Episode 134 || Total Reward : 37.581 || average reward : 25.050 || Used 53.985 seconds, mem : 400000\n",
      "Episode 135 || Total Reward : 36.032 || average reward : 25.336 || Used 53.642 seconds, mem : 400000\n",
      "Episode 136 || Total Reward : 35.568 || average reward : 25.615 || Used 53.658 seconds, mem : 400000\n",
      "Episode 137 || Total Reward : 36.873 || average reward : 25.906 || Used 53.542 seconds, mem : 400000\n",
      "Episode 138 || Total Reward : 36.237 || average reward : 26.198 || Used 53.847 seconds, mem : 400000\n",
      "Episode 139 || Total Reward : 36.013 || average reward : 26.496 || Used 53.762 seconds, mem : 400000\n",
      "Episode 140 || Total Reward : 35.074 || average reward : 26.775 || Used 54.142 seconds, mem : 400000\n",
      "Episode 141 || Total Reward : 31.659 || average reward : 27.021 || Used 53.655 seconds, mem : 400000\n",
      "Episode 142 || Total Reward : 31.205 || average reward : 27.242 || Used 53.843 seconds, mem : 400000\n",
      "Episode 143 || Total Reward : 27.195 || average reward : 27.434 || Used 53.555 seconds, mem : 400000\n",
      "Episode 144 || Total Reward : 27.637 || average reward : 27.634 || Used 53.779 seconds, mem : 400000\n",
      "Episode 145 || Total Reward : 29.708 || average reward : 27.861 || Used 53.763 seconds, mem : 400000\n",
      "Episode 146 || Total Reward : 31.104 || average reward : 28.105 || Used 53.811 seconds, mem : 400000\n",
      "Episode 147 || Total Reward : 30.697 || average reward : 28.336 || Used 53.797 seconds, mem : 400000\n",
      "Episode 148 || Total Reward : 27.478 || average reward : 28.537 || Used 53.596 seconds, mem : 400000\n",
      "Episode 149 || Total Reward : 30.268 || average reward : 28.766 || Used 53.687 seconds, mem : 400000\n",
      "\u001b[41mEpisode 150 || Total Reward : 30.792 || average reward : 28.991\n",
      "\u001b[0m\n",
      "Saved Networks in  trained_reacher_e150.pth\n",
      "Episode 151 || Total Reward : 32.749 || average reward : 29.220 || Used 53.940 seconds, mem : 400000\n",
      "Episode 152 || Total Reward : 33.377 || average reward : 29.458 || Used 53.736 seconds, mem : 400000\n",
      "Episode 153 || Total Reward : 30.599 || average reward : 29.655 || Used 53.998 seconds, mem : 400000\n",
      "Episode 154 || Total Reward : 31.699 || average reward : 29.860 || Used 53.775 seconds, mem : 400000\n",
      "Episode 155 || Total Reward : 32.255 || average reward : 30.073 || Used 54.321 seconds, mem : 400000\n",
      "Episode 156 || Total Reward : 34.629 || average reward : 30.298 || Used 54.146 seconds, mem : 400000\n",
      "Episode 157 || Total Reward : 32.826 || average reward : 30.487 || Used 54.322 seconds, mem : 400000\n",
      "Episode 158 || Total Reward : 32.796 || average reward : 30.676 || Used 54.849 seconds, mem : 400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 159 || Total Reward : 31.461 || average reward : 30.843 || Used 54.229 seconds, mem : 400000\n",
      "Episode 160 || Total Reward : 30.365 || average reward : 30.987 || Used 54.498 seconds, mem : 400000\n",
      "Episode 161 || Total Reward : 34.253 || average reward : 31.169 || Used 54.177 seconds, mem : 400000\n",
      "Episode 162 || Total Reward : 33.452 || average reward : 31.342 || Used 54.798 seconds, mem : 400000\n",
      "Episode 163 || Total Reward : 34.439 || average reward : 31.545 || Used 54.226 seconds, mem : 400000\n",
      "Episode 164 || Total Reward : 27.420 || average reward : 31.656 || Used 54.254 seconds, mem : 400000\n",
      "Episode 165 || Total Reward : 34.846 || average reward : 31.842 || Used 54.144 seconds, mem : 400000\n",
      "Episode 166 || Total Reward : 32.961 || average reward : 31.992 || Used 54.562 seconds, mem : 400000\n",
      "Episode 167 || Total Reward : 36.603 || average reward : 32.172 || Used 54.391 seconds, mem : 400000\n",
      "Episode 168 || Total Reward : 36.190 || average reward : 32.323 || Used 54.634 seconds, mem : 400000\n",
      "Episode 169 || Total Reward : 38.334 || average reward : 32.515 || Used 54.718 seconds, mem : 400000\n",
      "Episode 170 || Total Reward : 38.392 || average reward : 32.692 || Used 54.354 seconds, mem : 400000\n",
      "Episode 171 || Total Reward : 37.557 || average reward : 32.853 || Used 54.672 seconds, mem : 400000\n",
      "Episode 172 || Total Reward : 35.799 || average reward : 32.977 || Used 54.313 seconds, mem : 400000\n",
      "Episode 173 || Total Reward : 35.110 || average reward : 33.119 || Used 54.806 seconds, mem : 400000\n",
      "Episode 174 || Total Reward : 34.735 || average reward : 33.261 || Used 54.352 seconds, mem : 400000\n",
      "\u001b[41mEpisode 175 || Total Reward : 36.196 || average reward : 33.412\n",
      "\u001b[0m\n",
      "Episode 176 || Total Reward : 34.810 || average reward : 33.535 || Used 54.234 seconds, mem : 400000\n",
      "Episode 177 || Total Reward : 34.631 || average reward : 33.695 || Used 54.605 seconds, mem : 400000\n",
      "Episode 178 || Total Reward : 35.235 || average reward : 33.834 || Used 54.272 seconds, mem : 400000\n",
      "Episode 179 || Total Reward : 30.082 || average reward : 33.894 || Used 54.293 seconds, mem : 400000\n",
      "Episode 180 || Total Reward : 32.257 || average reward : 33.958 || Used 54.644 seconds, mem : 400000\n",
      "Episode 181 || Total Reward : 30.362 || average reward : 34.002 || Used 54.454 seconds, mem : 400000\n",
      "Episode 182 || Total Reward : 32.403 || average reward : 34.051 || Used 54.696 seconds, mem : 400000\n",
      "Episode 183 || Total Reward : 34.345 || average reward : 34.127 || Used 54.542 seconds, mem : 400000\n",
      "Episode 184 || Total Reward : 34.953 || average reward : 34.189 || Used 54.540 seconds, mem : 400000\n",
      "Episode 185 || Total Reward : 34.818 || average reward : 34.238 || Used 54.084 seconds, mem : 400000\n",
      "Episode 186 || Total Reward : 35.785 || average reward : 34.301 || Used 54.394 seconds, mem : 400000\n",
      "Episode 187 || Total Reward : 36.330 || average reward : 34.395 || Used 54.419 seconds, mem : 400000\n",
      "Episode 188 || Total Reward : 36.462 || average reward : 34.485 || Used 54.595 seconds, mem : 400000\n",
      "Episode 189 || Total Reward : 34.574 || average reward : 34.534 || Used 54.619 seconds, mem : 400000\n",
      "Episode 190 || Total Reward : 32.301 || average reward : 34.526 || Used 54.486 seconds, mem : 400000\n",
      "Episode 191 || Total Reward : 27.874 || average reward : 34.485 || Used 54.831 seconds, mem : 400000\n",
      "Episode 192 || Total Reward : 32.149 || average reward : 34.465 || Used 54.519 seconds, mem : 400000\n",
      "Episode 193 || Total Reward : 29.758 || average reward : 34.436 || Used 54.542 seconds, mem : 400000\n",
      "Episode 194 || Total Reward : 31.012 || average reward : 34.386 || Used 54.581 seconds, mem : 400000\n",
      "Episode 195 || Total Reward : 26.820 || average reward : 34.300 || Used 54.523 seconds, mem : 400000\n",
      "Episode 196 || Total Reward : 26.314 || average reward : 34.261 || Used 54.285 seconds, mem : 400000\n",
      "Episode 197 || Total Reward : 24.392 || average reward : 34.189 || Used 54.568 seconds, mem : 400000\n",
      "Episode 198 || Total Reward : 24.204 || average reward : 34.132 || Used 54.514 seconds, mem : 400000\n",
      "Episode 199 || Total Reward : 26.129 || average reward : 34.100 || Used 55.052 seconds, mem : 400000\n",
      "\u001b[41mEpisode 200 || Total Reward : 27.359 || average reward : 34.066\n",
      "\u001b[0m\n",
      "Saved Networks in  trained_reacher_e200.pth\n",
      "Episode 201 || Total Reward : 32.229 || average reward : 34.024 || Used 54.631 seconds, mem : 400000\n",
      "Episode 202 || Total Reward : 36.730 || average reward : 34.005 || Used 55.045 seconds, mem : 400000\n",
      "Episode 203 || Total Reward : 37.701 || average reward : 34.002 || Used 54.597 seconds, mem : 400000\n",
      "Episode 204 || Total Reward : 36.484 || average reward : 33.986 || Used 56.842 seconds, mem : 400000\n",
      "Episode 205 || Total Reward : 31.102 || average reward : 33.917 || Used 54.879 seconds, mem : 400000\n",
      "Episode 206 || Total Reward : 26.829 || average reward : 33.820 || Used 54.567 seconds, mem : 400000\n",
      "Episode 207 || Total Reward : 29.420 || average reward : 33.751 || Used 54.333 seconds, mem : 400000\n",
      "Episode 208 || Total Reward : 26.462 || average reward : 33.653 || Used 54.266 seconds, mem : 400000\n",
      "Episode 209 || Total Reward : 29.889 || average reward : 33.575 || Used 54.192 seconds, mem : 400000\n",
      "Episode 210 || Total Reward : 30.018 || average reward : 33.512 || Used 54.077 seconds, mem : 400000\n",
      "Episode 211 || Total Reward : 30.304 || average reward : 33.490 || Used 54.224 seconds, mem : 400000\n",
      "Episode 212 || Total Reward : 25.904 || average reward : 33.386 || Used 54.208 seconds, mem : 400000\n",
      "Episode 213 || Total Reward : 31.307 || average reward : 33.317 || Used 54.159 seconds, mem : 400000\n",
      "Episode 214 || Total Reward : 32.373 || average reward : 33.262 || Used 53.947 seconds, mem : 400000\n",
      "Episode 215 || Total Reward : 35.867 || average reward : 33.238 || Used 54.253 seconds, mem : 400000\n",
      "Episode 216 || Total Reward : 36.167 || average reward : 33.233 || Used 53.831 seconds, mem : 400000\n",
      "Episode 217 || Total Reward : 38.512 || average reward : 33.245 || Used 54.348 seconds, mem : 400000\n",
      "Episode 218 || Total Reward : 38.768 || average reward : 33.253 || Used 54.012 seconds, mem : 400000\n",
      "Episode 219 || Total Reward : 37.673 || average reward : 33.259 || Used 54.231 seconds, mem : 400000\n",
      "Episode 220 || Total Reward : 38.114 || average reward : 33.270 || Used 54.169 seconds, mem : 400000\n",
      "Episode 221 || Total Reward : 37.496 || average reward : 33.268 || Used 54.307 seconds, mem : 400000\n",
      "Episode 222 || Total Reward : 36.108 || average reward : 33.254 || Used 54.593 seconds, mem : 400000\n",
      "Episode 223 || Total Reward : 37.450 || average reward : 33.252 || Used 54.564 seconds, mem : 400000\n",
      "Episode 224 || Total Reward : 37.145 || average reward : 33.254 || Used 54.877 seconds, mem : 400000\n",
      "\u001b[41mEpisode 225 || Total Reward : 36.035 || average reward : 33.260\n",
      "\u001b[0m\n",
      "Episode 226 || Total Reward : 34.314 || average reward : 33.236 || Used 54.612 seconds, mem : 400000\n",
      "Episode 227 || Total Reward : 35.884 || average reward : 33.236 || Used 54.343 seconds, mem : 400000\n",
      "Episode 228 || Total Reward : 33.886 || average reward : 33.200 || Used 54.378 seconds, mem : 400000\n",
      "Episode 229 || Total Reward : 34.124 || average reward : 33.181 || Used 54.255 seconds, mem : 400000\n",
      "Episode 230 || Total Reward : 35.264 || average reward : 33.156 || Used 54.219 seconds, mem : 400000\n",
      "Episode 231 || Total Reward : 37.439 || average reward : 33.167 || Used 54.247 seconds, mem : 400000\n",
      "Episode 232 || Total Reward : 35.325 || average reward : 33.147 || Used 54.406 seconds, mem : 400000\n",
      "Episode 233 || Total Reward : 34.807 || average reward : 33.127 || Used 54.452 seconds, mem : 400000\n",
      "Episode 234 || Total Reward : 36.493 || average reward : 33.116 || Used 54.434 seconds, mem : 400000\n",
      "Episode 235 || Total Reward : 37.053 || average reward : 33.126 || Used 54.380 seconds, mem : 400000\n",
      "Episode 236 || Total Reward : 38.045 || average reward : 33.151 || Used 54.108 seconds, mem : 400000\n",
      "Episode 237 || Total Reward : 37.589 || average reward : 33.158 || Used 54.181 seconds, mem : 400000\n",
      "Episode 238 || Total Reward : 37.658 || average reward : 33.172 || Used 54.325 seconds, mem : 400000\n",
      "Episode 239 || Total Reward : 37.350 || average reward : 33.186 || Used 54.855 seconds, mem : 400000\n",
      "Episode 240 || Total Reward : 36.803 || average reward : 33.203 || Used 54.579 seconds, mem : 400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 241 || Total Reward : 37.850 || average reward : 33.265 || Used 54.858 seconds, mem : 400000\n",
      "Episode 242 || Total Reward : 37.155 || average reward : 33.324 || Used 54.343 seconds, mem : 400000\n",
      "Episode 243 || Total Reward : 33.509 || average reward : 33.387 || Used 54.482 seconds, mem : 400000\n",
      "Episode 244 || Total Reward : 33.413 || average reward : 33.445 || Used 54.651 seconds, mem : 400000\n",
      "Episode 245 || Total Reward : 35.789 || average reward : 33.506 || Used 54.846 seconds, mem : 400000\n",
      "Episode 246 || Total Reward : 37.139 || average reward : 33.566 || Used 54.820 seconds, mem : 400000\n",
      "Episode 247 || Total Reward : 38.314 || average reward : 33.643 || Used 54.631 seconds, mem : 400000\n",
      "Episode 248 || Total Reward : 38.364 || average reward : 33.751 || Used 54.933 seconds, mem : 400000\n",
      "Episode 249 || Total Reward : 38.733 || average reward : 33.836 || Used 54.630 seconds, mem : 400000\n",
      "\u001b[41mEpisode 250 || Total Reward : 37.585 || average reward : 33.904\n",
      "\u001b[0m\n",
      "Saved Networks in  trained_reacher_e250.pth\n",
      "Episode 251 || Total Reward : 35.318 || average reward : 33.930 || Used 55.037 seconds, mem : 400000\n",
      "Episode 252 || Total Reward : 36.900 || average reward : 33.965 || Used 54.651 seconds, mem : 400000\n",
      "Episode 253 || Total Reward : 36.038 || average reward : 34.019 || Used 54.511 seconds, mem : 400000\n",
      "Episode 254 || Total Reward : 36.468 || average reward : 34.067 || Used 54.562 seconds, mem : 400000\n",
      "Episode 255 || Total Reward : 34.933 || average reward : 34.094 || Used 54.639 seconds, mem : 400000\n"
     ]
    }
   ],
   "source": [
    "from ddpg_agent import ddpg_Agent\n",
    "import cProfile\n",
    "\n",
    "config = {\n",
    "    'gamma'               : 0.99,\n",
    "    'tau'                 : 1e-3,\n",
    "    'action_size'         : action_size,\n",
    "    'state_size'          : state_size,\n",
    "    'hidden_size'         : 256,\n",
    "    'buffer_size'         : 200000,\n",
    "    'batch_size'          : 256,\n",
    "    'dropout'             : 0.2,\n",
    "    'seed'                : 1,\n",
    "    'max_episodes'        : 256,\n",
    "    'critic_learning_rate': 1e-4,\n",
    "    'actor_learning_rate' : 1e-4,\n",
    "    'num_agents'          : num_agents,\n",
    "    'env_file_name'       : env_file_name,\n",
    "    'brain_name'          : brain_name}\n",
    "\n",
    "def print_config(config):\n",
    "    print('Config Parameters    : ')\n",
    "    for c,k in config.items():\n",
    "        print('{:20s} : {}'.format(c,k))\n",
    "        \n",
    "print_config(config)\n",
    "agent = ddpg_Agent(env, config)\n",
    "# cProfile.run(\"all_rewards = agent.train()\",'PerfStats')\n",
    "all_rewards = agent.train()\n",
    "\n",
    "if False:        \n",
    "    config_alt = dict(config)\n",
    "    config_alt['critic_learning_rate'] = 3e-4\n",
    "    print_config(config_alt)\n",
    "    agent = ddpg_Agent(env, config_alt)\n",
    "    all_rewards = agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "plt.ion()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(all_rewards)), all_rewards)\n",
    "plt.plot(np.arange(len(all_rewards)), 30*np.ones(len(all_rewards)))\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
